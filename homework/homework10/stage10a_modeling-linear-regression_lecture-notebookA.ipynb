{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8693e1f4",
   "metadata": {},
   "source": [
    "# Regression Assumptions Demonstration Notebook\n",
    "This notebook illustrates regression assumptions one by one with examples of violations and transformations/fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693502b9-4ef7-458e-8335-54938806666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf21e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe541a5c",
   "metadata": {},
   "source": [
    "## Function: Scatter plot with regression line and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a757060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ols(x, y):\n",
    "    \"\"\"Fits OLS regression and returns model and predicted values.\"\"\"\n",
    "    X_const = sm.add_constant(x)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    y_pred = model.predict(X_const)\n",
    "    return model, y_pred\n",
    "\n",
    "def plot_scatter_with_line(x, y, y_pred, r2, title=\"Scatter with Regression\", xlabel=\"X\", ylabel=\"Y\"):\n",
    "    \"\"\"Plots scatter plot and regression line with R^2.\"\"\"\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x, y, label=\"Data\")\n",
    "    plt.plot(x, y_pred, color='red', label=f\"Regression line\\n$R^2={r2:.2f}$\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def scatter_with_regression(x, y, title=\"Scatter with Regression\", xlabel=\"X\", ylabel=\"Y\"):\n",
    "    model, y_pred = fit_ols(x, y)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    plot_scatter_with_line(x, y, y_pred, r2, title, xlabel, ylabel)\n",
    "    return model\n",
    "    \n",
    "def scatter_with_regression_zoom(x, y, title=\"Scatter with Regression\", \n",
    "                                 xlabel=\"X\", ylabel=\"Y\", zoom=(0,0)):\n",
    "    \"\"\"\n",
    "    Fit OLS on full data, but optionally zoom in by removing extremes from plotting.\n",
    "    zoom = (n_x, n_y) removes n_x extreme points from x and n_y from y (top or bottom)\n",
    "    \"\"\"\n",
    "    # If x is a single-column DataFrame, convert to Series\n",
    "    if isinstance(x, pd.DataFrame) and x.shape[1] == 1:\n",
    "        x = x.iloc[:,0]\n",
    "        \n",
    "    # Fit full model\n",
    "    model, y_pred = fit_ols(x, y)\n",
    "    \n",
    "    # Determine indices to keep\n",
    "    idx_x = np.arange(len(x))\n",
    "    idx_y = np.arange(len(y))\n",
    "    \n",
    "    if zoom[0] > 0:\n",
    "        # Remove n_x most extreme x values (top or bottom)\n",
    "        sorted_x_idx = np.argsort(x)\n",
    "        idx_x = sorted_x_idx[zoom[0]:-zoom[0]] if zoom[0]*2 < len(x) else sorted_x_idx\n",
    "    if zoom[1] > 0:\n",
    "        # Remove n_y most extreme y values (top or bottom)\n",
    "        sorted_y_idx = np.argsort(y)\n",
    "        idx_y = sorted_y_idx[zoom[1]:-zoom[1]] if zoom[1]*2 < len(y) else sorted_y_idx\n",
    "    \n",
    "    # Intersection of indices to keep\n",
    "    keep_idx = np.intersect1d(idx_x, idx_y)\n",
    "    \n",
    "    # Filter for plotting only\n",
    "    x_plot = x.iloc[keep_idx] if isinstance(x, pd.Series) else x[keep_idx]\n",
    "    y_plot = y.iloc[keep_idx] if isinstance(y, pd.Series) else y[keep_idx]\n",
    "    y_pred_plot = y_pred[keep_idx]\n",
    "    \n",
    "    # Plot\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    plot_scatter_with_line(x_plot, y_plot, y_pred_plot, r2, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5b72e",
   "metadata": {},
   "source": [
    "## Function: Scatter two predictors side by side with regression lines from full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d790e4c-edec-4b55-918a-bb5ca7faf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_two_predictors_side_by_side_with_predictions(X, y, predictor1, predictor2):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
    "    \n",
    "    for i, col in enumerate([predictor1, predictor2]):\n",
    "        x = X[col]\n",
    "        X_const = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X_const).fit()\n",
    "        y_pred = model.predict(X_const)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        # Plot scatter and regression line\n",
    "        sorted_idx = x.argsort()\n",
    "        x_sorted = x.iloc[sorted_idx]\n",
    "        y_pred_sorted = y_pred.iloc[sorted_idx]\n",
    "        axes[i].scatter(x, y, label=\"Data\", alpha=0.6)\n",
    "        axes[i].plot(x_sorted, y_pred_sorted, color='red', label=f\"Regression line\\n$R^2={r2:.2f}$\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(\"Y\")\n",
    "        axes[i].set_title(f\"{col} vs Y (full model regression)\")\n",
    "        axes[i].legend()\n",
    "    \n",
    "    # Plot predicted vs observed Y\n",
    "    y_pred_full = sm.OLS(y, sm.add_constant(X)).fit().predict(sm.add_constant(X))\n",
    "    axes[2].scatter(y, y_pred_full, color='green', alpha=0.6, label=\"Predicted vs Observed\")\n",
    "    axes[2].plot([y.min(), y.max()], [y.min(), y.max()], color='black', linestyle='--', label='Perfect prediction')\n",
    "    axes[2].set_xlabel(\"Observed Y\")\n",
    "    axes[2].set_ylabel(\"Predicted Y\")\n",
    "    axes[2].set_title(\"Predicted vs Observed Y\")\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf00460",
   "metadata": {},
   "source": [
    "## Step 1: No Violation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac465ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X = pd.DataFrame({'X1': np.random.normal(0,1,n)})\n",
    "Y = 2*X['X1'] + np.random.normal(0,1,n)\n",
    "scatter_with_regression(X, Y, title=\"No Violation: Y ~ X1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085db98",
   "metadata": {},
   "source": [
    "## Step 2: Non-linearity (Y ~ X^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a027df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'X': np.random.uniform(-5,5,n)})\n",
    "Y = 0.5*X['X']**2 + np.random.normal(0,2,n)\n",
    "\n",
    "# Original non-linear\n",
    "scatter_with_regression(X, Y, title=\"Original Non-linear (Y ~ X^2)\")\n",
    "\n",
    "# Fix: regress only on X^2\n",
    "X_squared = pd.DataFrame({'X2': X['X']**2})\n",
    "scatter_with_regression(X_squared, Y, title=\"Regression on X^2 Only to Fix Non-linearity\",\n",
    "                        xlabel=\"X²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf14cd",
   "metadata": {},
   "source": [
    "## Step 3: Independence Violation (Autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros(n)\n",
    "for i in range(1,n):\n",
    "    Y[i] = 0.8*Y[i-1] + np.random.normal(0,1)\n",
    "X = pd.DataFrame({'X': np.arange(n)})\n",
    "\n",
    "scatter_with_regression(X, Y, title=\"Independence Violation (Autocorrelation)\")\n",
    "\n",
    "# Fix: difference transform\n",
    "Y_diff = np.diff(Y)\n",
    "X_diff = X['X'][1:]\n",
    "scatter_with_regression(X_diff, Y_diff, title=\"Differenced Y to Reduce Autocorrelation\",\n",
    "                        xlabel=\"X[1:]\", ylabel=\"ΔY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0870c",
   "metadata": {},
   "source": [
    "## Step 4: Heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(300)\n",
    "X = pd.DataFrame({'X': np.random.uniform(0,10,n)})\n",
    "Y = 2*X['X'] + np.random.normal(0, X['X'], n)  # increasing variance\n",
    "scatter_with_regression(X, Y, title=\"Heteroscedasticity Example\")\n",
    "\n",
    "# Fix: log-transform Y\n",
    "Y_log = np.log(Y - Y.min() + 1)\n",
    "scatter_with_regression(X, Y_log, title=\"Log-transformed Y to Stabilize Variance\",\n",
    "                        ylabel=\"log(Y - min(Y) + 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13138f3c",
   "metadata": {},
   "source": [
    "### Step 5: Non-normal residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X = pd.DataFrame({'X': np.random.normal(0,1,n)})\n",
    "Y = 2*X['X'] + np.random.exponential(scale=5, size=n)\n",
    "\n",
    "# Show non-normal residuals\n",
    "scatter_with_regression(X, Y, title=\"Non-normal Residuals (Exponential Noise)\")\n",
    "\n",
    "# Fix: shift so all Y > 0 before log-transform\n",
    "min_Y = Y.min()\n",
    "shift = 0.01 if min_Y >= 0 else -min_Y + 0.01\n",
    "Y_shifted = Y + shift\n",
    "Y_log = np.log1p(Y_shifted)\n",
    "\n",
    "scatter_with_regression(X, Y_log, title=\"Log1p-transformed Y (improved normality)\",\n",
    "                        ylabel=\"log1p(Y_shifted)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa1ebc",
   "metadata": {},
   "source": [
    "## Step 6: Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'X1': np.random.normal(0,1,n)})\n",
    "X['X2'] = X['X1']*0.9 + np.random.normal(0,0.1,n)  # highly correlated\n",
    "Y = 2*X['X1'] + 1.5*X['X2'] + np.random.normal(0,1,n)\n",
    "\n",
    "# scatter_two_predictors_side_by_side(X, Y, 'X1','X2')\n",
    "scatter_two_predictors_side_by_side_with_predictions(X, Y, 'X1','X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bed83-62dc-470d-b01a-fc036d28e0d4",
   "metadata": {},
   "source": [
    "# ⬆  \n",
    "**Triple Plot Explanation (Multicollinearity Example)**\n",
    "\n",
    "**Y vs X1**  \n",
    "- Red line: regression using all predictors (X1 & X2)  \n",
    "- You may see that the slope of the red line doesn’t match the raw correlation of X1 and Y, because the model has to “share” X1’s effect with X2.\n",
    "\n",
    "**Y vs X2**  \n",
    "- Same idea: slope might be surprising because X2 is correlated with X1.  \n",
    "- If X2 were included alone, the slope would be biased high, because it absorbs some of X1’s effect.\n",
    "\n",
    "**Predicted Y vs Observed Y**  \n",
    "- Even though coefficients are unstable, the predicted Y (green points) is close to actual Y → model predicts well.  \n",
    "\n",
    "**Takeaway:**  \n",
    "This illustrates a key multicollinearity point: good prediction doesn’t mean good coefficient estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ffa19",
   "metadata": {},
   "source": [
    "## Step 7: Outliers / Influential Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = pd.DataFrame({'X': np.random.normal(0,1,n)})\n",
    "Y = 2*X['X'] + np.random.normal(0,1,n)\n",
    "# Add outliers\n",
    "Y[0] += 170\n",
    "Y[1] -= 10\n",
    "scatter_with_regression(X, Y, title=\"Outliers / Influential Points\")\n",
    "\n",
    "# Plot first graph WITHOUT the high/low outliers\n",
    "# Y_out = Y.copy()\n",
    "# scatter_with_regression(X.iloc[1:], Y_out.iloc[1:], title=\"Outliers / Influential Points (excluded in plot)\")\n",
    "\n",
    "scatter_with_regression_zoom(X, Y, title=\"Zoomed view\", zoom=(0,2))\n",
    "\n",
    "# Fix: remove outliers\n",
    "X_clean = X.drop([0,1])\n",
    "Y_clean = Y.drop([0,1])\n",
    "scatter_with_regression(X_clean, Y_clean, title=\"After Removing Outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c46127",
   "metadata": {},
   "source": [
    "## Step 8: Omitted Variable Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee0ca5-da6b-48c0-b66c-c83128d31ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "# Generate predictors\n",
    "X1 = np.random.normal(0, 1, n)\n",
    "X2 = 2*X1 + np.sin(3*X1)  # correlated + patterned noise\n",
    "\n",
    "# True outcome\n",
    "Y = 2*X1 + 3*X2 + np.random.normal(0, 1, n)\n",
    "\n",
    "X = pd.DataFrame({'X1': X1, 'X2': X2})\n",
    "\n",
    "# Full model\n",
    "X_const = sm.add_constant(X)\n",
    "model_full = sm.OLS(Y, X_const).fit()\n",
    "y_pred_full = model_full.predict(X_const)\n",
    "\n",
    "# Single-variable regressions\n",
    "model_X1_only = sm.OLS(Y, sm.add_constant(X['X1'])).fit()\n",
    "y_pred_X1_only = model_X1_only.predict(sm.add_constant(X['X1']))\n",
    "\n",
    "model_X2_only = sm.OLS(Y, sm.add_constant(X['X2'])).fit()\n",
    "y_pred_X2_only = model_X2_only.predict(sm.add_constant(X['X2']))\n",
    "\n",
    "# Plot side by side plus predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
    "\n",
    "# --- Y vs X1 ---\n",
    "sorted_idx = X['X1'].argsort()\n",
    "axes[0].scatter(X['X1'], Y, alpha=0.6, label='Data')\n",
    "axes[0].plot(X['X1'].iloc[sorted_idx], y_pred_X1_only.iloc[sorted_idx], color='red', linewidth=2, label='Single-variable fit')\n",
    "axes[0].plot(X['X1'].iloc[sorted_idx], y_pred_full.iloc[sorted_idx], color='green', linewidth=2, linestyle='--', label='Full-model fit')\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Y vs X1')\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Y vs X2 ---\n",
    "sorted_idx = X['X2'].argsort()\n",
    "axes[1].scatter(X['X2'], Y, alpha=0.6, label='Data')\n",
    "axes[1].plot(X['X2'].iloc[sorted_idx], y_pred_X2_only.iloc[sorted_idx], color='red', linewidth=2, label='Single-variable fit')\n",
    "axes[1].plot(X['X2'].iloc[sorted_idx], y_pred_full.iloc[sorted_idx], color='green', linewidth=2, linestyle='--', label='Full-model fit')\n",
    "axes[1].set_xlabel('X2')\n",
    "axes[1].set_ylabel('Y')\n",
    "axes[1].set_title('Y vs X2')\n",
    "axes[1].legend()\n",
    "\n",
    "# --- Y_pred vs Y\n",
    "axes[2].scatter(Y, y_pred_X1_only, alpha=0.6, color='red', label='Predictions X1 only')\n",
    "axes[2].scatter(Y, y_pred_full, alpha=0.6, color='green', label='Predictions full model')\n",
    "axes[2].plot([Y.min(), Y.max()], [Y.min(), Y.max()], color='black', linestyle='--', label='Perfect prediction')\n",
    "axes[2].set_xlabel('Observed Y')\n",
    "axes[2].set_ylabel('Predicted Y')\n",
    "axes[2].set_title('Predicted vs Observed Y')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(model_X1_only.summary(),'\\n')\n",
    "print(model_X2_only.summary(),'\\n')\n",
    "print(model_full.summary(),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
